{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzbZ2sSWzdvi"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from progressbar import ProgressBar\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQOlP-aw8BQD"
      },
      "outputs": [],
      "source": [
        "data_dir = '/kaggle/input/daisee/DAiSEE/DataSet'\n",
        "label_dir = '/kaggle/input/daisee/DAiSEE/Labels'\n",
        "xception_weights_dir = '/kaggle/input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "resnet_weights_dir = '/kaggle/input/resnet50-weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "vgg16_weights_dir = '/kaggle/input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "frames_dir = '/kaggle/working/frames'\n",
        "numpy_dir = '/kaggle/working/labels'\n",
        "model_dir = '/kaggle/working/model12'\n",
        "evaluate_dir = \"/kaggle/working/evaluate\"\n",
        "history_dir = '/kaggle/working/history'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK_RFTeP8Ezl"
      },
      "source": [
        "# Извлечение кадров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxafBKbF8Dic"
      },
      "outputs": [],
      "source": [
        "def get_frames(subdirectory, video, odir):\n",
        "    subprocess.run(f\"ffmpeg -i {video} -vf fps=0.7 \"\n",
        "                    f\"{odir}/{video.parts[-1][:-4]}_%1d.jpeg \"\n",
        "                    \"-loglevel quiet\", shell=True, check=True)\n",
        "\n",
        "def extract_frames(data_dir, label_dir, out_dir):\n",
        "    data_dir = Path(data_dir)\n",
        "    label_dir = Path(label_dir)\n",
        "    out_dir = Path(out_dir)\n",
        "\n",
        "    subdirectories = [\"Train\", \"Test\", \"Validation\"]\n",
        "    for subdirectory in subdirectories:\n",
        "        sdir = data_dir / subdirectory\n",
        "        label_path = str(label_dir) + f\"/{subdirectory}Labels.csv\"\n",
        "        odir = out_dir / subdirectory\n",
        "        odir.mkdir(parents=True, exist_ok=True)\n",
        "        label = pd.read_csv(label_path)\n",
        "        print(f\"Extracting frames for {subdirectory}\")\n",
        "        with ProgressBar(max_value=len(list(sdir.glob(\"*/*/*\")))) as bar:\n",
        "            for i, video in enumerate(sdir.glob(\"*/*/*\")):\n",
        "                if label['ClipID'].str.contains(video.parts[-1]).any():\n",
        "                    get_frames(subdirectory, video, odir)\n",
        "                bar.update(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRkgbtc78O2H"
      },
      "outputs": [],
      "source": [
        "extract_frames(data_dir, label_dir, frames_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0aOnIVg8Rwy"
      },
      "source": [
        "# Сохранение заголовков и меток"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGNo22CT8T7h"
      },
      "outputs": [],
      "source": [
        "def save_filepath_label(usage, frame_dir, label_dir, out_dir):\n",
        "    frame_dir = Path(frame_dir)\n",
        "    label_dir = Path(label_dir)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    frame_dir = frame_dir / usage\n",
        "    print(type(frame_dir))\n",
        "\n",
        "    label_path = str(label_dir) + f\"/{usage}Labels.csv\"\n",
        "\n",
        "    labeldf = pd.read_csv(label_path)\n",
        "    nrows = len(list(frame_dir.glob(\"*.jpeg\")))\n",
        "    ncols = len(labeldf.columns) - 1\n",
        "    filepath = np.empty((nrows,), dtype=object)\n",
        "    label = np.empty((nrows, ncols), dtype=np.float32)\n",
        "    print(f\"Getting filepath and labels for {usage}\")\n",
        "    with ProgressBar(max_value=nrows) as bar:\n",
        "        for i, frame in enumerate(frame_dir.glob(\"*.jpeg\")):\n",
        "            filepath[i] = str(frame)\n",
        "            framename = frame.parts[-1]\n",
        "            frameid = framename[:framename.find(\"_\")]\n",
        "            video = frameid + \".avi\"\n",
        "            if labeldf['ClipID'].str.contains(video).any():\n",
        "                lidx = labeldf.index[labeldf['ClipID'].str.contains(video)]\n",
        "            else:\n",
        "                video = frameid + \".mp4\"\n",
        "                lidx = labeldf.index[labeldf['ClipID'].str.contains(video)]\n",
        "            label[i] = labeldf.iloc[lidx, 1:]\n",
        "            bar.update(i)\n",
        "\n",
        "    np.random.seed(100)\n",
        "    indices = np.random.permutation(nrows)\n",
        "    filepath = filepath[indices]\n",
        "    print(filepath[0])\n",
        "    label = label[indices]\n",
        "    np.save(f\"{str(out_dir)}/x_{usage.lower()}\", filepath, allow_pickle=True)\n",
        "    np.save(f\"{str(out_dir)}/y_{usage.lower()}\", label)\n",
        "    return filepath, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fXxBHP-8XVz"
      },
      "outputs": [],
      "source": [
        "save_filepath_label(\"Train\", frame_dir, label_dir, numpy_dir)\n",
        "save_filepath_label(\"Test\", frame_dir, label_dir, numpy_dir)\n",
        "save_filepath_label(\"Validation\", frame_dir, label_dir, numpy_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fUPSg5j8aVv"
      },
      "outputs": [],
      "source": [
        "class_names = np.array(\n",
        "    ['Boredom', 'Engagement', 'Confusion', 'Frustration']\n",
        ")\n",
        "autotune = tf.data.experimental.AUTOTUNE\n",
        "img_width = 299\n",
        "img_height = 299\n",
        "batch_size = 64\n",
        "shuffle_buffer_size = 3000\n",
        "old_epoch = 0\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "finetune_at = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDHbQ1Ud8cIH"
      },
      "outputs": [],
      "source": [
        "def show_batch(image, label):\n",
        "    image = image.numpy()\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(6, 6, i + 1)\n",
        "        imgtitle = [label[\"y1\"][i].numpy().item(),\n",
        "                    label[\"y2\"][i].numpy().item(),\n",
        "                    label[\"y3\"][i].numpy().item(),\n",
        "                    label[\"y4\"][i].numpy().item()]\n",
        "        plt.imshow(np.uint8(image[i] * 255))\n",
        "        plt.title(imgtitle, fontsize=8)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_function(filepath, label):\n",
        "    image = tf.io.read_file(filepath)\n",
        "    image = tf.io.decode_jpeg(contents=image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image=image, dtype=tf.float32)\n",
        "    image = tf.image.resize(images=image,\n",
        "                            size=[img_width, img_height],\n",
        "                            method=tf.image.ResizeMethod.BILINEAR,\n",
        "                            antialias=True)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dataset(usage, numpy_dir):\n",
        "    numpy_dir = Path(numpy_dir)\n",
        "    x = np.load(numpy_dir / f'x_{usage.lower()}.npy', allow_pickle=True)\n",
        "    y = np.load(numpy_dir / f'y_{usage.lower()}.npy')\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (x, {\"y1\": y[:, :1], \"y2\": y[:, 1:2],\n",
        "             \"y3\": y[:, 2:3], \"y4\": y[:, :3:4]})\n",
        "    )\n",
        "    dataset = dataset.map(map_func=parse_function, num_parallel_calls=autotune)\n",
        "    if usage == 'Train':\n",
        "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size,\n",
        "                                  reshuffle_each_iteration=True)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(autotune)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeNbGIRW8gtQ"
      },
      "outputs": [],
      "source": [
        "train_set = get_dataset(\"Train\", numpy_dir)\n",
        "test_set = get_dataset(\"Test\", numpy_dir)\n",
        "val_set = get_dataset(\"Validation\", numpy_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4204oDF08kNx"
      },
      "source": [
        "# Обучение ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCggETJQ8ja7"
      },
      "outputs": [],
      "source": [
        "def get_ResNet50_model(weight_dir, out_dir, fullyconnected=False, finetune=False):\n",
        "    if finetune:\n",
        "        if fullyconnected:\n",
        "            base_model = load_model(str(out_dir) + \"/ResNet50_on_DAiSEE_fc.h5\")\n",
        "        else:\n",
        "            base_model = load_model(str(out_dir) + \"/ResNet50_on_DAiSEE.h5\")\n",
        "\n",
        "        base_model.trainable = True\n",
        "        for layer in base_model.layers[:finetune_at]:\n",
        "            layer.trainable = False\n",
        "        return base_model\n",
        "    else:\n",
        "        resnet50 = ResNet50(weights=Path(weight_dir), include_top=False, input_shape=(img_width, img_height, 3))\n",
        "        resnet50.trainable = False\n",
        "\n",
        "        x = GlobalAveragePooling2D()(resnet50.output)\n",
        "\n",
        "        if fullyconnected:\n",
        "            x = Dense(128, activation=\"relu\", name=\"fc1\")(x)\n",
        "            x = Dense(64, activation=\"relu\", name=\"fc2\")(x)\n",
        "\n",
        "        boredom = Dense(4, name=\"y1\")(x)\n",
        "        engagement = Dense(4, name=\"y2\")(x)\n",
        "        confusion = Dense(4, name=\"y3\")(x)\n",
        "        frustration = Dense(4, name=\"y4\")(x)\n",
        "\n",
        "        model = Model(inputs=resnet50.input, outputs=[boredom, engagement, confusion, frustration])\n",
        "    return model\n",
        "\n",
        "def ResNet50_train(weight_dir, numpy_dir, out_dir, history_dir, fullyconnected=False, finetune=False):\n",
        "    global old_epoch\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    log_dir = out_dir / \"logs\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_ds = get_dataset(\"Train\", numpy_dir)\n",
        "    validation_ds = get_dataset(\"Validation\", numpy_dir)\n",
        "    model = get_ResNet50_model(weight_dir, out_dir, fullyconnected, finetune)\n",
        "\n",
        "    if finetune:\n",
        "        lr = base_learning_rate / 10\n",
        "        if fullyconnected:\n",
        "            model_path = str(out_dir) + \"/ResNet50_on_DAiSEE_finetune_fc.h5\"\n",
        "            history_path = str(history_dir) + \"/resnet_training_history_finetune_fc.pkl\"\n",
        "        else:\n",
        "            model_path = str(out_dir) + \"/ResNet50_on_DAiSEE_finetune.h5\"\n",
        "            history_path = str(history_dir) + \"/resnet_training_history_finetune.pkl\"\n",
        "\n",
        "    else:\n",
        "        lr = base_learning_rate\n",
        "        if fullyconnected:\n",
        "            model_path = str(out_dir) + \"/ResNet50_on_DAiSEE_fc.h5\"\n",
        "            history_path = str(history_dir) + \"/resnet_training_history_fc.pkl\"\n",
        "        else:\n",
        "            model_path = str(out_dir) + \"/ResNet50_on_DAiSEE.h5\"\n",
        "            history_path = str(history_dir) + \"/resnet_training_history.pkl\"\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=lr),\n",
        "                  loss={\"y1\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y2\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y3\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y4\": SparseCategoricalCrossentropy(from_logits=True)},\n",
        "                  metrics={\"y1\": \"sparse_categorical_accuracy\",\n",
        "                           \"y2\": \"sparse_categorical_accuracy\",\n",
        "                           \"y3\": \"sparse_categorical_accuracy\",\n",
        "                           \"y4\": \"sparse_categorical_accuracy\"})\n",
        "    print(model.summary())\n",
        "\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', min_delta=1e-2, patience=2, verbose=1),\n",
        "                 TensorBoard(log_dir=str(log_dir))\n",
        "    ]\n",
        "\n",
        "    history = model.fit(train_ds,\n",
        "                        steps_per_epoch=len(train_ds),\n",
        "                        epochs=1,\n",
        "                        initial_epoch=old_epoch,\n",
        "                        validation_data=validation_ds,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_steps=len(validation_ds))\n",
        "\n",
        "    model.save(model_path)\n",
        "    with open(history_path, 'wb') as file:\n",
        "        pickle.dump(history.history, file)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNXx8s1j8pCo"
      },
      "outputs": [],
      "source": [
        "model, history = ResNet50_train(resnet_weights_dir, numpy_dir, model_dir, history_dir, fullyconnected=False, finetune=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI3QE6h78psi"
      },
      "outputs": [],
      "source": [
        "model, history = ResNet50_train(resnet_weights_dir, numpy_dir, model_dir, history_dir, fullyconnected=True, finetune=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NDnLDsW8tR1"
      },
      "source": [
        "# Обучение Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsP5V8Dq8t2o"
      },
      "outputs": [],
      "source": [
        "def get_Xception_model(weight_dir, out_dir, fullyconnected=False, finetune=False):\n",
        "    if finetune:\n",
        "        if fullyconnected:\n",
        "            base_model = load_model(str(out_dir) + \"/Xception_on_DAiSEE_fc.h5\")\n",
        "        else:\n",
        "            base_model = load_model(str(out_dir) + \"/Xception_on_DAiSEE.h5\")\n",
        "\n",
        "        base_model.trainable = True\n",
        "        for layer in base_model.layers[:finetune_at]:\n",
        "            layer.trainable = False\n",
        "        return base_model\n",
        "    else:\n",
        "        base_model = Xception(weights=weight_dir,\n",
        "                              include_top=False,\n",
        "                              input_shape=(img_width, img_height, 3))\n",
        "\n",
        "        base_model.trainable = False\n",
        "        x = GlobalAveragePooling2D()(base_model.output)\n",
        "        if fullyconnected:\n",
        "            x = Dense(128, activation=\"relu\", name=\"fc1\")(x)\n",
        "            x = Dense(64, activation=\"relu\", name=\"fc2\")(x)\n",
        "        boredom = Dense(4, name=\"y1\")(x)\n",
        "        engagement = Dense(4, name=\"y2\")(x)\n",
        "        confusion = Dense(4, name=\"y3\")(x)\n",
        "        frustration = Dense(4, name=\"y4\")(x)\n",
        "        model = Model(inputs=base_model.input, outputs=[boredom, engagement, confusion, frustration])\n",
        "    return model\n",
        "\n",
        "\n",
        "def Xception_train(weight_dir, numpy_dir, out_dir, fullyconnected=False, finetune=False):\n",
        "    global old_epoch\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    log_dir = out_dir / \"logs\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_ds = get_dataset(\"Train\", numpy_dir).take(48)\n",
        "    validation_ds = get_dataset(\"Validation\", numpy_dir).take(16)\n",
        "    model = get_Xception_model(weight_dir, out_dir, fullyconnected, finetune)\n",
        "\n",
        "    if finetune:\n",
        "        lr = base_learning_rate / 10\n",
        "        finetune_epochs = 0\n",
        "        if fullyconnected:\n",
        "            model_path = str(out_dir) + \"/Xception_on_DAiSEE_finetune_fc.h5\"\n",
        "            history_path = \"/kaggle/working/history/xception_training_history_finetune_fc.pkl\"\n",
        "        else:\n",
        "            model_path = str(out_dir) + \"/Xception_on_DAiSEE_finetune.h5\"\n",
        "            history_path = \"/kaggle/working/history/xception_training_history_finetune.pkl\"\n",
        "    else:\n",
        "        lr = base_learning_rate\n",
        "        finetune_epochs = 0\n",
        "        if fullyconnected:\n",
        "            model_path = str(out_dir) + \"/Xception_on_DAiSEE_fc.h5\"\n",
        "            history_path = \"/kaggle/working/history/xception_training_history_fc.pkl\"\n",
        "        else:\n",
        "            model_path = str(out_dir) + \"/Xception_on_DAiSEE.h5\"\n",
        "            history_path = \"/kaggle/working/history/xception_training_history.pkl\"\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=lr),\n",
        "                  loss={\"y1\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y2\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y3\": SparseCategoricalCrossentropy(from_logits=True),\n",
        "                        \"y4\": SparseCategoricalCrossentropy(from_logits=True)},\n",
        "                  metrics={\"y1\": \"sparse_categorical_accuracy\",\n",
        "                           \"y2\": \"sparse_categorical_accuracy\",\n",
        "                           \"y3\": \"sparse_categorical_accuracy\",\n",
        "                           \"y4\": \"sparse_categorical_accuracy\"})\n",
        "    print(model.summary())\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', min_delta=1e-2,\n",
        "                      patience=2, verbose=1),\n",
        "        TensorBoard(log_dir=str(log_dir))\n",
        "    ]\n",
        "\n",
        "    total_epochs = epochs + finetune_epochs\n",
        "    history = model.fit(train_ds,\n",
        "                        epochs=total_epochs,\n",
        "                        initial_epoch=old_epoch,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_data=validation_ds)\n",
        "\n",
        "    model.save(model_path)\n",
        "\n",
        "\n",
        "    with open(history_path, 'wb') as file:\n",
        "        pickle.dump(history.history, file)\n",
        "\n",
        "    if finetune:\n",
        "        old_epoch = 0\n",
        "    else:\n",
        "        old_epoch = history.epoch[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ZwbILr8xti"
      },
      "outputs": [],
      "source": [
        "model, history = Xception_train(xception_weights_dir, numpy_dir, model_dir, history_dir, fullyconnected=False, finetune=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCrZbjR68zo0"
      },
      "outputs": [],
      "source": [
        "model, history = Xception_train(xception_weights_dir, numpy_dir, model_dir, history_dir, fullyconnected=True, finetune=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zyFQlCw81uJ"
      },
      "source": [
        "# Оценка моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_UyBzku84GE"
      },
      "outputs": [],
      "source": [
        "resnet_model = load_model(f\"/kaggle/working/model12/ResNet50_on_DAiSEE.h5\")\n",
        "resnet_accuracy = resnet_model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwmoLhyp86QM"
      },
      "outputs": [],
      "source": [
        "resnet_model = load_model(f\"/kaggle/working/model12/ResNet50_on_DAiSEE_fc.h5\")\n",
        "resnet_accuracy = resnet_model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp8f09h788dP"
      },
      "outputs": [],
      "source": [
        "xception_model = load_model(f\"/kaggle/working/model/Xception_on_DAiSEE.h5\")\n",
        "xception_accuracy = xception_model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwBXt7O38-ZU"
      },
      "outputs": [],
      "source": [
        "xception_model = load_model(f\"/kaggle/working/model/Xception_on_DAiSEE_fc.h5\")\n",
        "xception_accuracy = xception_model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06KZgexX9AEH"
      },
      "outputs": [],
      "source": [
        "xception_model = load_model(f\"/kaggle/working/model/Xception_on_DAiSEE_finetune.h5\")\n",
        "xception_accuracy = xception_model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvcgqU2t9B3W"
      },
      "outputs": [],
      "source": [
        "xception_model = load_model(f\"/kaggle/working/model/Xception_on_DAiSEE_finetune_fc.h5\")\n",
        "xception_accuracy = xception_model.evaluate(test_set)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
